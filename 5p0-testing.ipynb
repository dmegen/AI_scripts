{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "import neptune_tensorboard as neptune_tb\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tensorflow.keras.optimizers import *\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch > 0:\n",
    "        return 0.01 / epoch\n",
    "    else:\n",
    "        return 0.01\n",
    "\n",
    "#\n",
    "# Mean Absolute Error metric\n",
    "#\n",
    "def mae(y_true, y_pred):\n",
    "  eval = K.abs(y_pred - y_true)\n",
    "  eval = K.mean(eval, axis=-1)\n",
    "  return eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATC_PARAMS = {'epochs': 10,\n",
    "                'beta_1': 0.9,\n",
    "                'beta_2': 0.999,\n",
    "                'epsilon': 1.0,\n",
    "                'decay': 0.0,\n",
    "                'momentum': 0.9,\n",
    "          }\n",
    "PARAMS = {'epochs': 10,\n",
    "          'batch_size': 32,\n",
    "          'optimizer': 'Adam',\n",
    "          'learning_rate': 0.01,\n",
    "          'beta_1': 0.9,\n",
    "          'beta_2': 0.999,\n",
    "          'epsilon': 1.0,\n",
    "          'decay': 0.0,\n",
    "          'momentum': 0.9,\n",
    "          }\n",
    "\n",
    "if PARAMS['optimizer'] == 'Adam':\n",
    "  optimizer = Adam(lr=PARAMS['learning_rate'],\n",
    "                   beta_1=PARAMS['beta_1'],\n",
    "                   beta_2=PARAMS['beta_2'],\n",
    "                   epsilon=PARAMS['epsilon'],\n",
    "                   decay=PARAMS['decay']\n",
    "  )\n",
    "elif PARAMS['optimizer'] == 'SGD':\n",
    "  optimizer = SGD(lr=PARAMS['learning_rate'],\n",
    "                  decay=PARAMS['decay'],\n",
    "                  momentum=PARAMS['momentum'],\n",
    "                  nesterov=True\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root_directory = '/glade/work/hardt/models'\n",
    "model_run_name        = 'unet_v5p0'\n",
    "from unet_model_v5p0 import unet\n",
    "\n",
    "#\n",
    "# Altitude in meters to run\n",
    "#\n",
    "feature_description = '0to6.5km_at_500m_steps'\n",
    "\n",
    "# \n",
    "# 1)     0 meters AGL\n",
    "# 2)   500\n",
    "# 3)  1000 \n",
    "# 4)  1500 \n",
    "# 5)  2000\n",
    "# 6)  2500\n",
    "# 7)  3000\n",
    "# 15) 7000 meters AGL\n",
    "#\n",
    "levels = {}\n",
    "level_count = 1\n",
    "for i in range(0,7500,500):\n",
    "    label_name = str(i)\n",
    "    levels[label_name] = level_count\n",
    "    level_count = level_count + 1\n",
    "\n",
    "level_label = '5500'\n",
    "label_level = levels[level_label]\n",
    "#--------------------------\n",
    "\n",
    "load_previous_model = False\n",
    "previous_model = 'trained_model_feature-0to6.5km_at_500m_steps_label-5500m_2020_11_19_17_32.h5'\n",
    "input_model = os.path.join(output_root_directory,model_run_name, previous_model)\n",
    "\n",
    "#--------------------------\n",
    "\n",
    "output_model_name     = 'trained_model_feature-' + feature_description + '_label-' + level_label + 'm_{}.h5'\n",
    "log_dir = os.path.join(output_root_directory, model_run_name, 'logs', 'fit',output_model_name.format(t))\n",
    "feature_data          = '/glade/work/hardt/ds612/2000-2013_June-Sept_QRAIN_INTERP_AGL_0to7km_at_500m_steps.nc'\n",
    "label_data            = '/glade/work/hardt/ds612/2000-2013_June-Sept_W_INTERP_AGL_0to7km_at_500m_steps.nc'\n",
    "\n",
    "BATCH_SIZE = PARAMS['batch_size']\n",
    "epochs = PARAMS['epochs']\n",
    "\n",
    "data_fraction_for_training = 0.65\n",
    "data_fraction_for_validation = 0.25\n",
    "\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of images: 9568\n",
      "Training data start image: 0\n",
      "Training data end image: 6208\n",
      "Valication data start image: 6209\n",
      "Validation data end image: 8609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(output_root_directory, model_run_name)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "#\n",
    "# load the data\n",
    "#\n",
    "fds = xr.open_dataset(feature_data)\n",
    "lds = xr.open_dataset(label_data)\n",
    "feature = fds.QRAIN.values\n",
    "label = lds.W.values\n",
    "\n",
    "#\n",
    "# move the channels from position 1 to position 3\n",
    "# goes from [time,channel,height,width] to [time, height, width, channel]\n",
    "# which is the default for Conv2D.\n",
    "#\n",
    "feature = np.moveaxis(feature, 1, 3)\n",
    "label = np.moveaxis(label, 1, 3)\n",
    "\n",
    "label[feature<.01] = -99.0\n",
    "\n",
    "#\n",
    "# random shuffle \n",
    "#\n",
    "# s = np.arange(feature.shape[0])\n",
    "# np.random.shuffle(s)\n",
    "\n",
    "#\n",
    "# \n",
    "#\n",
    "num_images = feature.shape[0]\n",
    "\n",
    "train_data_start = 0\n",
    "train_data_end   = int( num_images * data_fraction_for_training  / BATCH_SIZE ) * BATCH_SIZE\n",
    "\n",
    "val_data_start = train_data_end + 1\n",
    "val_data_end = int(  ( num_images * (data_fraction_for_training + data_fraction_for_validation) - val_data_start)  / BATCH_SIZE )\n",
    "val_data_end = (val_data_end * BATCH_SIZE) + val_data_start\n",
    "\n",
    "print ()\n",
    "print (\"Number of images:\", num_images)\n",
    "print (\"Training data start image:\", train_data_start)\n",
    "print (\"Training data end image:\", train_data_end)\n",
    "print (\"Valication data start image:\", val_data_start)\n",
    "print (\"Validation data end image:\", val_data_end)\n",
    "print ()\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = train_data_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEPTUNE_API_TOKEN = os.environ.get('NEPTUNE_API_TOKEN')\n",
    "#neptune.init(project_qualified_name='hardt/Predicting-W',\n",
    "#             api_token=NEPTUNE_API_TOKEN)\n",
    "#neptune.create_experiment(name='v5p0 14channel-W-5.5km 0-100 epochs', \n",
    "#                          params=PARAMS,\n",
    "#                          tags=['v5p0', 'Adam', 'LRS', 'Shuffle']\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = tf.data.Dataset.from_tensor_slices((feature[s][train_data_start:train_data_end,:,:,:14], label[s][train_data_start:train_data_end,:,:,label_level]))\n",
    "#val_dataset   = tf.data.Dataset.from_tensor_slices((feature[s][val_data_start:val_data_end,:,:,:14], label[s][val_data_start:val_data_end,:,:,label_level]))\n",
    "#\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((feature[train_data_start:train_data_end,:,:,:14], label[train_data_start:train_data_end,:,:,label_level]))\n",
    "val_dataset   = tf.data.Dataset.from_tensor_slices((feature[val_data_start:val_data_end,:,:,:14], label[val_data_start:val_data_end,:,:,label_level]))\n",
    "#\n",
    "#\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "#\n",
    "# set up the model\n",
    "#\n",
    "output_model = os.path.join(output_path, output_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unet_model_v5p0\n",
    "import importlib\n",
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256, 256, 14)\n",
      "(None, 256, 256, 1)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 14 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 64) 8128        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 128 147584      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 256)  590080      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 512)  2359808     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32, 32, 512)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 1024) 4719616     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 1024) 9438208     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 16, 1024) 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 32, 32, 1024) 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 1024) 0           dropout[0][0]                    \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 512)  0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 64, 64, 512)  0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 128, 128, 256 0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 128, 128 295040      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 128, 128 147584      conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 128 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 256, 256, 128 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 2)  1154        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 1)  3           conv2d_22[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,039,173\n",
      "Trainable params: 31,039,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(unet_model_v5p0)\n",
    "\n",
    "model = unet_model_v5p0.unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(256, 256, 14), dtype=float32)\n",
      "[<tf.Tensor 'conv2d_23/BiasAdd:0' shape=(None, 256, 256, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(model.input[0])\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_loss(y_true, y_pred, x_true):\n",
    "\n",
    "    print(x_true.shape)\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "    #mask = tf.math.greater(x_true, 0.01)\n",
    "    \n",
    "    y_pred = y_pred[x_true>0.01]\n",
    "    y_true = y_true[x_true>0.01]\n",
    "\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    huber = tf.keras.losses.Huber()\n",
    "    \n",
    "    return huber(y_true, y_pred)\n",
    "\n",
    "def cust_loss(x_true):\n",
    "    def loss(y_true, y_pred):\n",
    "        return thresh_loss(y_true, y_pred, x_true)\n",
    "    return loss\n",
    "\n",
    "def get_loss_fcn():\n",
    "    def loss_fcn(y_true, y_pred):\n",
    "        print(y_true.shape)\n",
    "        print(y_pred.shape)\n",
    "        x_true = y_pred[:,:,:,12]\n",
    "        y_pred = y_pred[:,:,:,0]\n",
    "        \n",
    "        print(tf.keras.backend.shape(x_true))\n",
    "        print(tf.keras.backend.shape(y_true))\n",
    "        print(tf.keras.backend.shape(y_pred))\n",
    "        \n",
    "#        y_pred = tf.keras.backend.concatenate([y_pred, y_pred], axis=3)\n",
    "#        x_true = tf.keras.backend.concatenate([x_true, x_true], axis=3)\n",
    "        return thresh_loss(y_true, y_pred, x_true)\n",
    "    return loss_fcn\n",
    "\n",
    "#loss = get_loss_fcn()\n",
    "\n",
    "def ref_only_loss(y_true, y_pred, thresh):\n",
    "\n",
    "    y_true[y_true>=thresh]\n",
    "    y_pred[y_true>=thresh]\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    huber = tf.keras.losses.Huber()\n",
    "    return mse(y_true, y_pred)\n",
    "\n",
    "def refl_loss(thresh):\n",
    "    def ref(y_true, y_pred):\n",
    "        return ref_only_loss(y_true, y_pred, thresh)\n",
    "    return ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=optimizer, loss=thresh_loss, metrics = ['accuracy','mae'], run_eagerly=True)\n",
    "model.compile(optimizer=optimizer, loss=refl_loss(0.1), metrics = ['accuracy','mae'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_save_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/glade/scratch/hardt/unet_v1/trained_model_epoch{epoch}.h5',save_freq='epoch')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(output_path,\"trained_weights_best_\" + level_label + \"AGL.h5\"), monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "LRS = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  1/194 [..............................] - ETA: 0s - loss: 9780.5508 - accuracy: 2.8610e-06 - mae: 98.7962WARNING:tensorflow:From /glade/work/hardt/20191211_20200420/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "194/194 [==============================] - ETA: 0s - loss: 9774.5605 - accuracy: 2.6718e-06 - mae: 98.1591\n",
      "Epoch 00001: accuracy improved from -inf to 0.00000, saving model to /glade/work/hardt/models/unet_v5p0/trained_weights_best_5500AGL.h5\n",
      "194/194 [==============================] - 189s 973ms/step - loss: 9774.5605 - accuracy: 2.6718e-06 - mae: 98.1591 - val_loss: 9426.5410 - val_accuracy: 2.9246e-06 - val_mae: 97.0117\n",
      "Epoch 2/5\n",
      "194/194 [==============================] - ETA: 0s - loss: 9247.2480 - accuracy: 2.6718e-06 - mae: 96.0957\n",
      "Epoch 00002: accuracy did not improve from 0.00000\n",
      "194/194 [==============================] - 190s 979ms/step - loss: 9247.2480 - accuracy: 2.6718e-06 - mae: 96.0957 - val_loss: 9063.0928 - val_accuracy: 2.9246e-06 - val_mae: 95.1263\n",
      "Epoch 3/5\n",
      "194/194 [==============================] - ETA: 0s - loss: 8976.2656 - accuracy: 2.6718e-06 - mae: 94.6804\n",
      "Epoch 00003: accuracy did not improve from 0.00000\n",
      "194/194 [==============================] - 190s 980ms/step - loss: 8976.2656 - accuracy: 2.6718e-06 - mae: 94.6804 - val_loss: 8884.4580 - val_accuracy: 2.9246e-06 - val_mae: 94.1857\n",
      "Epoch 4/5\n",
      "194/194 [==============================] - ETA: 0s - loss: 8827.7412 - accuracy: 2.6718e-06 - mae: 93.8951\n",
      "Epoch 00004: accuracy did not improve from 0.00000\n",
      "194/194 [==============================] - 190s 980ms/step - loss: 8827.7412 - accuracy: 2.6718e-06 - mae: 93.8951 - val_loss: 8766.2451 - val_accuracy: 2.9246e-06 - val_mae: 93.5581\n",
      "Epoch 5/5\n",
      "194/194 [==============================] - ETA: 0s - loss: 8724.4229 - accuracy: 2.6718e-06 - mae: 93.3449\n",
      "Epoch 00005: accuracy did not improve from 0.00000\n",
      "194/194 [==============================] - 190s 978ms/step - loss: 8724.4229 - accuracy: 2.6718e-06 - mae: 93.3449 - val_loss: 8677.9912 - val_accuracy: 2.9246e-06 - val_mae: 93.0868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b514ff50350>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset, callbacks=[tensorboard, LRS, checkpoint, NeptuneMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "model.save(output_model.format(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
