{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "import neptune_tensorboard as neptune_tb\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tensorflow.keras.optimizers import *\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch > 0:\n",
    "        return 0.01 / epoch\n",
    "    else:\n",
    "        return 0.01\n",
    "\n",
    "#\n",
    "# Mean Absolute Error metric\n",
    "#\n",
    "def mae(y_true, y_pred):\n",
    "  eval = K.abs(y_pred - y_true)\n",
    "  eval = K.mean(eval, axis=-1)\n",
    "  return eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    coef = (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATC_PARAMS = {'epochs': 5,\n",
    "                'beta_1': 0.9,\n",
    "                'beta_2': 0.999,\n",
    "                'epsilon': 1.0,\n",
    "                'decay': 0.0,\n",
    "                'momentum': 0.9,\n",
    "          }\n",
    "PARAMS = {'epochs': 10,\n",
    "          'batch_size': 32,\n",
    "          'optimizer': 'Adam',\n",
    "          'learning_rate': 0.01,\n",
    "          'beta_1': 0.9,\n",
    "          'beta_2': 0.999,\n",
    "          'epsilon': 1.0,\n",
    "          'decay': 0.0,\n",
    "          'momentum': 0.9,\n",
    "          'custLossThresh': -99.0,\n",
    "          'refl_scaling_min': -35.0,\n",
    "          'refl_scaling_per99.99': 58.3864573,\n",
    "          'W_scaling_min': -13.606483,\n",
    "          'W_scaling_per99.99': 1.2770988,\n",
    "          }\n",
    "\n",
    "if PARAMS['optimizer'] == 'Adam':\n",
    "  optimizer = Adam(lr=PARAMS['learning_rate'],\n",
    "                   beta_1=PARAMS['beta_1'],\n",
    "                   beta_2=PARAMS['beta_2'],\n",
    "                   epsilon=PARAMS['epsilon'],\n",
    "                   decay=PARAMS['decay']\n",
    "  )\n",
    "elif PARAMS['optimizer'] == 'SGD':\n",
    "  optimizer = SGD(lr=PARAMS['learning_rate'],\n",
    "                  decay=PARAMS['decay'],\n",
    "                  momentum=PARAMS['momentum'],\n",
    "                  nesterov=True\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root_directory = '/glade/work/hardt/models'\n",
    "model_run_name        = 'unet_v6p0'\n",
    "from unet_model_v6p0 import unet\n",
    "\n",
    "#\n",
    "# Altitude in meters to run\n",
    "#\n",
    "feature_description = '5minAfterHour_refl'\n",
    "\n",
    "#--------------------------\n",
    "\n",
    "load_previous_model = False\n",
    "previous_model = 'trained_model_feature-0to6.5km_at_500m_steps_label-5500m_2020_11_19_17_32.h5'\n",
    "input_model = os.path.join(output_root_directory,model_run_name, previous_model)\n",
    "\n",
    "#--------------------------\n",
    "\n",
    "output_model_name     = 'trained_model_feature-' + feature_description + '_{}.h5'\n",
    "log_dir = os.path.join(output_root_directory, model_run_name, 'logs', 'fit',output_model_name.format(t))\n",
    "feature_data          = '/glade/work/hardt/ds612/model2_5minuteAfterHour_3D_refl_shuffled.nc'\n",
    "label_data            = '/glade/work/hardt/ds612/model2_3D_W_shuffled.nc'\n",
    "\n",
    "BATCH_SIZE = PARAMS['batch_size']\n",
    "epochs = PARAMS['epochs']\n",
    "custLossThresh = PARAMS['custLossThresh']\n",
    "\n",
    "data_fraction_for_training = 0.65\n",
    "data_fraction_for_validation = 0.25\n",
    "\n",
    "label_level = 0\n",
    "\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_root_directory, model_run_name)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "#\n",
    "# load the data\n",
    "#\n",
    "fds = xr.open_dataset(feature_data)\n",
    "lds = xr.open_dataset(label_data)\n",
    "feature = fds.REFL_10CM.values\n",
    "#label = lds.W.values\n",
    "label = lds.W.values.max(axis=1)\n",
    "label = label[:,:,:,np.newaxis]\n",
    "\n",
    "#\n",
    "# move the channels from position 1 to position 3\n",
    "# goes from [time,channel,height,width] to [time, height, width, channel]\n",
    "# which is the default for Conv2D.\n",
    "#\n",
    "feature = np.moveaxis(feature, 1, 3)\n",
    "#label = np.moveaxis(label, 1, 3)\n",
    "\n",
    "#\n",
    "# Based on the min and max from scaling the data\n",
    "# V = Vs * (refl_scaling_per99.99 - refl_scaling_min) - refl_scaling_min\n",
    "#   = Vs * 93.3865 + -35\n",
    "# a value of 0.5 would be ~11dbz\n",
    "# a value of 0.6 would be ~20dbz\n",
    "# a value of 0.7 would be ~30dbz\n",
    "# \n",
    "# \n",
    "feature2D = np.amax(feature, axis=3)\n",
    "feature2D = feature2D[:,:,:,np.newaxis]\n",
    "#for i in range(label.shape[3]):\n",
    "#    label[:,:,:,i] = label[:,:,:,i][feature2D[:,:,:,0]<0.4] = -99.0\n",
    "    \n",
    "label[feature2D<0.4] = -99.0\n",
    "\n",
    "#\n",
    "# random shuffle \n",
    "#\n",
    "# s = np.arange(feature.shape[0])\n",
    "# np.random.shuffle(s)\n",
    "\n",
    "#\n",
    "# \n",
    "#\n",
    "num_images = feature.shape[0]\n",
    "\n",
    "train_data_start = 0\n",
    "train_data_end   = int( num_images * data_fraction_for_training  / BATCH_SIZE ) * BATCH_SIZE\n",
    "\n",
    "val_data_start = train_data_end + 1\n",
    "val_data_end = int(  ( num_images * (data_fraction_for_training + data_fraction_for_validation) - val_data_start)  / BATCH_SIZE )\n",
    "val_data_end = (val_data_end * BATCH_SIZE) + val_data_start\n",
    "\n",
    "print ()\n",
    "print (\"Number of images:\", num_images)\n",
    "print (\"Training data start image:\", train_data_start)\n",
    "print (\"Training data end image:\", train_data_end)\n",
    "print (\"Validation data start image:\", val_data_start)\n",
    "print (\"Validation data end image:\", val_data_end)\n",
    "print ()\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = train_data_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPTUNE_API_TOKEN = os.environ.get('NEPTUNE_API_TOKEN')\n",
    "neptune.init(project_qualified_name='hardt/Pred-W-RefOffset',\n",
    "             api_token=NEPTUNE_API_TOKEN)\n",
    "neptune.create_experiment(name='v6p0 ref 5min after 0-100 epochs', \n",
    "                          params=PARAMS,\n",
    "                          tags=['v6p0', 'Adam', 'LRS', 'Shuffle','CustomLoss']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# \n",
    "#\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((feature[train_data_start:train_data_end,:,:,:], label[train_data_start:train_data_end,:,:,label_level]))\n",
    "val_dataset   = tf.data.Dataset.from_tensor_slices((feature[val_data_start:val_data_end,:,:,:], label[val_data_start:val_data_end,:,:,label_level]))\n",
    "#\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "#\n",
    "# set up the model\n",
    "#\n",
    "output_model = os.path.join(output_path, output_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unet_model_v6p0\n",
    "import importlib\n",
    "mse = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(unet_model_v6p0)\n",
    "\n",
    "model = unet_model_v6p0.unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input[0])\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresh_loss(y_true, y_pred, x_true):\n",
    "\n",
    "    print(x_true.shape)\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "    #mask = tf.math.greater(x_true, 0.01)\n",
    "    \n",
    "    y_pred = y_pred[x_true>0.01]\n",
    "    y_true = y_true[x_true>0.01]\n",
    "\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    huber = tf.keras.losses.Huber()\n",
    "    \n",
    "    return huber(y_true, y_pred)\n",
    "\n",
    "def cust_loss(x_true):\n",
    "    def loss(y_true, y_pred):\n",
    "        return thresh_loss(y_true, y_pred, x_true)\n",
    "    return loss\n",
    "\n",
    "def get_loss_fcn():\n",
    "    def loss_fcn(y_true, y_pred):\n",
    "        print(y_true.shape)\n",
    "        print(y_pred.shape)\n",
    "        x_true = y_pred[:,:,:,12]\n",
    "        y_pred = y_pred[:,:,:,0]\n",
    "        \n",
    "        print(tf.keras.backend.shape(x_true))\n",
    "        print(tf.keras.backend.shape(y_true))\n",
    "        print(tf.keras.backend.shape(y_pred))\n",
    "        \n",
    "#        y_pred = tf.keras.backend.concatenate([y_pred, y_pred], axis=3)\n",
    "#        x_true = tf.keras.backend.concatenate([x_true, x_true], axis=3)\n",
    "        return thresh_loss(y_true, y_pred, x_true)\n",
    "    return loss_fcn\n",
    "\n",
    "#loss = get_loss_fcn()\n",
    "\n",
    "def custom_loss(y_true, y_pred, thresh):\n",
    "\n",
    "    y_true[y_true!=thresh]\n",
    "    y_pred[y_true!=thresh]\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    huber = tf.keras.losses.Huber()\n",
    "    return mse(y_true, y_pred)\n",
    "\n",
    "def cust_loss(thresh):\n",
    "    def loss(y_true, y_pred):\n",
    "        return custom_loss(y_true, y_pred, thresh)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=optimizer, loss=thresh_loss, metrics = ['accuracy','mae'], run_eagerly=True)\n",
    "model.compile(optimizer=optimizer, loss=cust_loss(custLossThresh), metrics = ['accuracy','mae','dice_coef'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_save_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/glade/scratch/hardt/unet_v1/trained_model_epoch{epoch}.h5',save_freq='epoch')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(output_path,\"trained_weights_best.h5\"), monitor='dice_coef', verbose=1, save_best_only=True, mode='max')\n",
    "LRS = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[tensorboard, LRS, checkpoint, NeptuneMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "model.save(output_model.format(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
