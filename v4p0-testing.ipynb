{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tensorflow.keras.optimizers import *\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from unet_model_v4p0 import unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {'epochs': 2,\n",
    "          'batch_size': 32,\n",
    "          'optimizer': 'Adam',\n",
    "          'learning_rate': 1e-6,\n",
    "          'beta_1': 0.9,\n",
    "          'beta_2': 0.999,\n",
    "          'epsilon': 1.0,\n",
    "          'decay': 0.0,\n",
    "          'momentum': 0.9,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "\n",
    "output_root_directory = '/glade/work/hardt/models'\n",
    "model_run_name        = 'unet_v4p0'\n",
    "from unet_model_v4p0 import unet\n",
    "\n",
    "#\n",
    "# Altitude in meters to run\n",
    "#\n",
    "feature_description = '0to6.5km_at_500m_steps'\n",
    "\n",
    "levels = {}\n",
    "level_count = 1\n",
    "for i in range(0,7500,500):\n",
    "    label_name = str(i)\n",
    "    levels[label_name] = level_count\n",
    "    level_count = level_count + 1\n",
    "\n",
    "level_label = '5500'\n",
    "label_level = levels[level_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_previous_model = False\n",
    "previous_model = 'trained_model_feature-0to6.5km_at_500m_steps_label-5500m_2020_11_19_17_32.h5'\n",
    "input_model = os.path.join(output_root_directory,model_run_name, previous_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_name     = 'trained_model_feature-' + feature_description + '_label-' + level_label + 'm_{}.h5'\n",
    "log_dir = os.path.join(output_root_directory, model_run_name, 'logs', 'fit',output_model_name.format(t))\n",
    "feature_data          = '/glade/work/hardt/ds612/2000-2013_June-Sept_QRAIN_INTERP_AGL_0to7km_at_500m_steps.nc'\n",
    "label_data            = '/glade/work/hardt/ds612/2000-2013_June-Sept_W_INTERP_AGL_0to7km_at_500m_steps.nc'\n",
    "\n",
    "BATCH_SIZE = PARAMS['batch_size']\n",
    "epochs = PARAMS['epochs']\n",
    "\n",
    "data_fraction_for_training = 0.65\n",
    "data_fraction_for_validation = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(output_root_directory, model_run_name)\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# load the data\n",
    "#\n",
    "fds = xr.open_dataset(feature_data)\n",
    "lds = xr.open_dataset(label_data)\n",
    "feature = fds.QRAIN.values\n",
    "label = lds.W.values\n",
    "\n",
    "#\n",
    "# move the channels from position 1 to position 3\n",
    "# goes from [time,channel,height,width] to [time, height, width, channel]\n",
    "# which is the default for Conv2D.\n",
    "#\n",
    "feature = np.moveaxis(feature, 1, 3)\n",
    "label = np.moveaxis(label, 1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = np.arange(feature.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(s)\n",
    "#print(feature[s][1,1,1,1])\n",
    "#print(label[s][1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# \n",
    "#\n",
    "num_images = feature.shape[0]\n",
    "\n",
    "train_data_start = 0\n",
    "train_data_end   = int( num_images * data_fraction_for_training  / BATCH_SIZE ) * BATCH_SIZE\n",
    "\n",
    "val_data_start = train_data_end + 1\n",
    "val_data_end = int(  ( num_images * (data_fraction_for_training + data_fraction_for_validation) - val_data_start)  / BATCH_SIZE )\n",
    "val_data_end = (val_data_end * BATCH_SIZE) + val_data_start\n",
    "\n",
    "print ()\n",
    "print (\"Number of images:\", num_images)\n",
    "print (\"Training data start image:\", train_data_start)\n",
    "print (\"Training data end image:\", train_data_end)\n",
    "print (\"Validation data start image:\", val_data_start)\n",
    "print (\"Validation data end image:\", val_data_end)\n",
    "print ()\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = train_data_end\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((feature[train_data_start:train_data_end,:,:,:14], label[train_data_start:train_data_end,:,:,label_level]))\n",
    "val_dataset   = tf.data.Dataset.from_tensor_slices((feature[val_data_start:val_data_end,:,:,:14], label[val_data_start:val_data_end,:,:,label_level]))\n",
    "#\n",
    "#\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set up the model\n",
    "#\n",
    "output_model = os.path.join(output_path, output_model_name)\n",
    "\n",
    "# if load_previous_model:\n",
    "#   model = tf.keras.models.load_model(input_model, compile=False)\n",
    "# else:\n",
    "#   model = unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "  if epoch < 6:\n",
    "    return 0.0001\n",
    "  else:\n",
    "    return 0.0001 * tf.math.exp(0.1 * (10 - epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(output_path,\"trained_weights_best_\" + level_label + \"AGL.h5\"), monitor='mae', verbose=1, save_best_only=True, mode='min')\n",
    "LRS = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=optimizer, loss=cust_loss(0.01), metrics = ['accuracy','mae'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_dataset, epochs=epochs, validation_data=val_dataset, callbacks=[tensorboard, checkpoint, NeptuneMonitor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([256, 512]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.5, 0.6))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam','sgd','rmsprop']))\n",
    "HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(.001,.01))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER,HP_L2],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fraction_for_test = 0.10\n",
    "\n",
    "test_data_start = int(num_images * (1 - data_fraction_for_test))\n",
    "test_data_start = (num_images - int((num_images - test_data_start) / BATCH_SIZE) * BATCH_SIZE) \n",
    "test_data_end = num_images\n",
    "\n",
    "print ()\n",
    "print (\"Number of images:\", num_images)\n",
    "print (\"Test data start image:\", test_data_start)\n",
    "print (\"Test data end image:\", test_data_end)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((feature[test_data_start:test_data_end,:,:,:14]))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "\n",
    "    from unet_model_test import unet\n",
    "\n",
    "    model = unet(hparams)\n",
    "    \n",
    "    model.compile(\n",
    "      optimizer=hparams[HP_OPTIMIZER],\n",
    "      loss='MeanSquaredError',\n",
    "      metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    model.fit(train_dataset, epochs=2, validation_data=val_dataset) \n",
    "    _, accuracy = model.evaluate(test_dataset)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "    for l2 in (HP_L2.domain.min_value, HP_L2.domain.max_value):\n",
    "      for optimizer in HP_OPTIMIZER.domain.values:\n",
    "        hparams = {\n",
    "            HP_NUM_UNITS: num_units,\n",
    "            HP_DROPOUT: dropout_rate,\n",
    "            HP_L2: l2,\n",
    "            HP_OPTIMIZER: optimizer,\n",
    "        }\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "        run(log_dir +'/hparam_tuning/' + run_name, hparams)\n",
    "        session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
